{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Name:** Omar Khaled Mahmoud Safwat\r\n",
    "\r\n",
    "\r\n",
    "**Group:** Alex. G3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd \r\n",
    "\r\n",
    "from sklearn import datasets\r\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\r\n",
    "\r\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "\r\n",
    "iris = datasets.load_iris()\r\n",
    "X = iris[\"data\"]\r\n",
    "y = iris[\"target\"]\r\n",
    "df = pd.DataFrame({fname: values for fname, values in zip(iris[\"feature_names\"], X.T)})\r\n",
    "df[\"target\"] = y\r\n",
    "\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mini-Batch GD"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Softmax Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class Softmax():\r\n",
    "    \"\"\"Class Implements Softmax Regresson for multinomial classification\"\"\"\r\n",
    "    \r\n",
    "    def __init__(self):\r\n",
    "        self.X_train = None # User input data.\r\n",
    "        self.y = None # Target feature\r\n",
    "        self.y_ohe = None # Target Variable One-hot encoded\r\n",
    "        self.weights = None # weights of model\r\n",
    "\r\n",
    "    def encode_target(self, y):\r\n",
    "        \"\"\"One-hot encode target variable\"\"\"\r\n",
    "\r\n",
    "        assert y.ndim == 1, \"Target variable should be 1 dimensional\"\r\n",
    "\r\n",
    "        y_ohe = np.zeros((len(y), y.max() + 1))\r\n",
    "        y_ohe[np.arange(len(y)), y] = 1 \r\n",
    "        return y_ohe\r\n",
    "\r\n",
    "    def hypothesis(self, X=None, weights=None, batch_indices=None):\r\n",
    "        \"\"\"Hypothesis function\"\"\"\r\n",
    "        if X is None:\r\n",
    "            X = self.X\r\n",
    "        if weights is None:\r\n",
    "            weights = self.weights\r\n",
    "        if batch_indices is None:\r\n",
    "            batch_indices = list(range(len(self.y)))\r\n",
    "            \r\n",
    "        score_fun = X[batch_indices, :] @ weights # Score function for each class, for each sample\r\n",
    "        score_exp = np.exp(score_fun)\r\n",
    "\r\n",
    "        # Returns a 2D Numpy array, prediction of each class for each sample\r\n",
    "        # Shape: (m, k)\r\n",
    "        # m: Number of samples, k: Number of classes\r\n",
    "        return(score_exp / np.sum(score_exp, axis=1, keepdims=True))\r\n",
    "        \r\n",
    "    def loss(self, h, batch_indices):\r\n",
    "        \"\"\"Cost function\"\"\"\r\n",
    "        y_ohe = self.y_ohe[batch_indices, :]\r\n",
    "        h += 1e-7 # Add tolerance term for the log\r\n",
    "        return -np.sum(y_ohe * np.log(h))\r\n",
    "\r\n",
    "    def loss_prime(self, h, batch_indices):\r\n",
    "        \"\"\"Jacobian vector of cost function\"\"\"\r\n",
    "        y_ohe = self.y_ohe[batch_indices, :]\r\n",
    "        X = self.X[batch_indices, :]\r\n",
    "\r\n",
    "        return (X.T @ (h - y_ohe))\r\n",
    "\r\n",
    "    def initialize(self, learn_rate, batch_size):\r\n",
    "        \"\"\"\r\n",
    "        Initialize first epoch\r\n",
    "        \r\n",
    "        Args:\r\n",
    "            learn_rate (float): Learning rate for gradient descent\r\n",
    "\r\n",
    "            guess (int, 2D numpy array): Initial guess for model weights\r\n",
    "\r\n",
    "            batch_size (int): Training set batch size \r\n",
    "        \"\"\"\r\n",
    "        # Initialize guesses\r\n",
    "        self.weights_hist = np.random.randn(self.X.shape[1], self.y_ohe.shape[1], 1) # Initialize weight vector, account for bias column\r\n",
    "\r\n",
    "        # Initialize history\r\n",
    "        self.loss_hist = np.empty(shape=(1))\r\n",
    "        self.grad_hist = np.empty_like(self.weights_hist)\r\n",
    "\r\n",
    "        self.n_points = batch_size\r\n",
    "        self.epoch = 0\r\n",
    "        self.learn_rate = learn_rate\r\n",
    "\r\n",
    "    def update_weights_GD(self, idx_1, idx_2, weights):\r\n",
    "        \"\"\"\r\n",
    "        Function updates weights along the direction of steepest descent and stores results\r\n",
    "        \r\n",
    "        Args:\r\n",
    "            idx_1 (int): Start index for the current batch\r\n",
    "\r\n",
    "            idx_2 (int): End index for the current batch (non inclusive)\r\n",
    "\r\n",
    "            weights (2D numpy array): Model weights   \r\n",
    "        \"\"\"\r\n",
    "    \r\n",
    "        # Compute predictions\r\n",
    "        self.h_pred = self.hypothesis(batch_indices=self.train_indx[idx_1: idx_2], weights=weights)\r\n",
    "\r\n",
    "        # Compute weights gradient\r\n",
    "        grad_new = self.loss_prime(self.h_pred, self.train_indx[idx_1: idx_2])\r\n",
    "        self.grad_hist = np.dstack((self.grad_hist, np.atleast_3d(grad_new)))\r\n",
    "\r\n",
    "        # Update Weights\r\n",
    "        weights = weights - self.learn_rate * grad_new\r\n",
    "        self.weights_hist = np.dstack((self.weights_hist, np.atleast_3d(weights)))\r\n",
    "\r\n",
    "    def mini_batch_GD(self, learn_rate=0.01, n_batches=8, max_epochs=1e3, seed=None):\r\n",
    "        \"\"\"\r\n",
    "        Optimize weights using Mini_batch Gradient Descent\r\n",
    "        \r\n",
    "        Args:\r\n",
    "\r\n",
    "            learn_rate (float, optional): Learning rate for gradient descent\r\n",
    "            \r\n",
    "            n_batches (int, optional): Number of batches for mini-batch Gradient descent\r\n",
    "\r\n",
    "            max_epochs (int, optional): Maximum number of iterations over the entire training set\r\n",
    "\r\n",
    "            seed (int, optional): seed the the data shuffeling after before each epoch\r\n",
    "\r\n",
    "        Returns:\r\n",
    "\r\n",
    "            2d numpy array: Trained model weights\r\n",
    "        \"\"\"\r\n",
    "        # Randomly shuffle the dataset's order\r\n",
    "        if (seed is not None):\r\n",
    "            np.random.seed(seed)\r\n",
    "\r\n",
    "        self.n_batches = n_batches\r\n",
    "        np.random.shuffle(self.train_indx)\r\n",
    "        self.MAX_EPOCHS = max_epochs\r\n",
    "\r\n",
    "        batch_size = len(self.train_indx) // n_batches\r\n",
    "        \r\n",
    "        # Initialize first epoch\r\n",
    "        self.initialize(learn_rate, batch_size)\r\n",
    "        \r\n",
    "        # Train model using Mini_batches\r\n",
    "        while (self.epoch < self.MAX_EPOCHS):\r\n",
    "            for i in range(n_batches):\r\n",
    "                idx_1 = i * batch_size\r\n",
    "                idx_2 = idx_1 + batch_size\r\n",
    "                self.update_weights_GD(idx_1, idx_2, self.weights_hist[:, :, -1])\r\n",
    "            \r\n",
    "            # Update Predictions and Loss from last epoch\r\n",
    "            self.h_pred = self.hypothesis(batch_indices=self.train_indx[:batch_size], weights=self.weights_hist[:, :, -1])\r\n",
    "            self.loss_hist = np.append(self.loss_hist, self.loss(self.h_pred, self.train_indx[:batch_size]))\r\n",
    "            \r\n",
    "            # Increment epoch and shuffle data incides for next epoch\r\n",
    "            self.epoch += 1\r\n",
    "            np.random.shuffle(self.train_indx)\r\n",
    "\r\n",
    "            # Record minimum loss so far if early stopping is used\r\n",
    "            if self.early_stop == True:\r\n",
    "                # Check if stop criteria threshold is met\r\n",
    "                if (self.epoch - self.best_epoch) > self.early_stop_thresh:\r\n",
    "                    self.is_converged = True\r\n",
    "                    return self.weights_hist[:, :, -1]        \r\n",
    "                    \r\n",
    "                val_preds = self.hypothesis(batch_indices=self.valid_indx, weights=self.weights_hist[:, :, -1])\r\n",
    "                current_loss = self.loss(val_preds, self.valid_indx)\r\n",
    "                if self.min_val_loss > current_loss:\r\n",
    "                    self.min_val_loss = current_loss\r\n",
    "                    self.best_epoch = self.epoch\r\n",
    "                \r\n",
    "\r\n",
    "        return self.weights_hist[:, :, -1]\r\n",
    "\r\n",
    "    def fit(self, X, y, early_stop=False, early_stop_thresh=5, **kwargs):\r\n",
    "        \"\"\"Fit model to training data\"\"\"\r\n",
    "\r\n",
    "        # X, and y are 2D Numpy arrays.\r\n",
    "        # Add a column of ones for theta_0\r\n",
    "        self.X = np.hstack((np.ones((len(X), 1)), X))\r\n",
    "        self.y = y\r\n",
    "\r\n",
    "\r\n",
    "        # One-hot encode target variable\r\n",
    "        self.y_ohe = self.encode_target(y)\r\n",
    "        all_indices = np.array([i for i in range(len(self.X))])\r\n",
    "        \r\n",
    "        self.early_stop = early_stop\r\n",
    "\r\n",
    "        # Split the data to validation set to check early stop\r\n",
    "        if self.early_stop == True:\r\n",
    "            validation_mask = np.array([np.random.choice([0, 1], p=[0.7, 0.3]) for i in range(len(y))], dtype='bool')\r\n",
    "            self.valid_indx = all_indices[validation_mask]\r\n",
    "            self.train_indx = all_indices[~validation_mask]\r\n",
    "\r\n",
    "            self.min_val_loss = float(\"inf\")           # Value of minimum loss\r\n",
    "            self.best_epoch = 0                        # Epoch with minimum loss\r\n",
    "            self.early_stop_thresh = early_stop_thresh # Maximum number of epochs to perform without improvement in loss\r\n",
    "            self.is_converged = False\r\n",
    "\r\n",
    "        else:    \r\n",
    "            self.train_indx = all_indices\r\n",
    "\r\n",
    "        self.weights = self.mini_batch_GD(**kwargs)\r\n",
    "\r\n",
    "    def predict(self, X):\r\n",
    "        \"\"\"Returns predictions\"\"\"\r\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\r\n",
    "        proba = self.hypothesis(X, self.weights, list(range(X.shape[0])))\r\n",
    "        return np.argmax(proba, axis=1)\r\n",
    "\r\n",
    "    def predict_proba(self, X):\r\n",
    "        \"\"\"Returns predictions for each class as probabilities\"\"\"\r\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\r\n",
    "        return self.hypothesis(X, self.weights, list(range(X.shape[0])))\r\n",
    "\r\n",
    "    def accuracy_score(self, X_test, y_test):\r\n",
    "        \"\"\"Returns model accuracy score\"\"\"\r\n",
    "        return np.sum(self.predict(X_test) == y_test) / len(y_test)\r\n",
    "    \r\n",
    "    def show_summary(self):\r\n",
    "        \"\"\"Prints a brief summary after stop criteria is reached\"\"\"\r\n",
    "        print(\"Solver summary:\")\r\n",
    "        print(\"=\" * len(\"Solver summary:\"))\r\n",
    "\r\n",
    "        if self.early_stop == True:\r\n",
    "            print(\"Number of epochs: \", self.best_epoch)\r\n",
    "            print(\"Negative Log Likelihood: \", self.loss_hist[self.best_epoch])\r\n",
    "            print(\"Early stop criteria was reached first: \", self.is_converged)\r\n",
    "            print(\"Train accuracy: \", self.accuracy_score(self.X[self.train_indx, 1:], self.y[self.train_indx]))\r\n",
    "            print(\"Validation accuracy: \", self.accuracy_score(self.X[self.valid_indx, 1:], self.y[self.valid_indx]))\r\n",
    "        else:\r\n",
    "            print(\"Number of epochs: \", self.epoch)\r\n",
    "            print(\"Negative Log Likelihood: \", self.loss_hist[-1])\r\n",
    "            print(\"Train accuracy: \", self.accuracy_score(self.X[self.train_indx, 1:], self.y[self.train_indx]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train with Mini_batch GD\r\n",
    "## Without Early stop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Split dataset into validation and training\r\n",
    "all_indices = np.array([i for i in range(len(X))])\r\n",
    "np.random.shuffle(all_indices)\r\n",
    "validation_mask = np.array([np.random.choice([0, 1], p=[0.7, 0.3]) for i in range(len(y))], dtype='bool')\r\n",
    "train_indx, val_indx = all_indices[~validation_mask], all_indices[validation_mask]\r\n",
    "\r\n",
    "X_train, y_train = X[train_indx], y[train_indx]\r\n",
    "X_val, y_val = X[val_indx], y[val_indx]\r\n",
    "\r\n",
    "softmax = Softmax()\r\n",
    "softmax.fit(X_train, y_train, early_stop=False, learn_rate=0.05)\r\n",
    "softmax.show_summary()\r\n",
    "print('Validation accuracy: ', softmax.accuracy_score(X_val, y_val))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Solver summary:\n",
      "===============\n",
      "Number of epochs:  1000\n",
      "Negative Log Likelihood:  0.5375553478186633\n",
      "Train accuracy:  0.9904761904761905\n",
      "Validation accuracy:  0.9777777777777777\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## With Early stop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "softmax = Softmax()\r\n",
    "softmax.fit(X, y, early_stop=True, early_stop_thresh=300, learn_rate=0.05)\r\n",
    "softmax.show_summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Solver summary:\n",
      "===============\n",
      "Number of epochs:  66\n",
      "Negative Log Likelihood:  0.37147882094226636\n",
      "Early stop criteria was reached first:  True\n",
      "Train accuracy:  0.979381443298969\n",
      "Validation accuracy:  0.9622641509433962\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cross Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class KfoldCV():\r\n",
    "  \"\"\"Class implements k-fold cross validation\"\"\"\r\n",
    "\r\n",
    "  def __init__(self, model):\r\n",
    "      self.model = model # ML model object\r\n",
    "      self.accuracy_scores=None\r\n",
    "\r\n",
    "  def fit(self, X, y, n_folds=3, **kwargs):\r\n",
    "    \"\"\"Train machine learning model using k_folds cross validation\"\"\"\r\n",
    "    \r\n",
    "    self.accuracy_scores = []\r\n",
    "\r\n",
    "    # Split Dataset\r\n",
    "    split = StratifiedShuffleSplit(n_splits=n_folds, test_size=0.2, random_state=42)\r\n",
    "    for train_index, test_index in split.split(X, y):\r\n",
    "        X_train, y_train = X[train_index], y[train_index]\r\n",
    "        X_test, y_test = X[test_index], y[test_index]\r\n",
    "\r\n",
    "        self.model.fit(X_train, y_train, **kwargs)\r\n",
    "        self.accuracy_scores.append(self.model.accuracy_score(X_test, y_test))\r\n",
    "\r\n",
    "  def predict(self, X):\r\n",
    "    \"\"\"Return model predictions for dataset X\"\"\"\r\n",
    "    return self.model.predict(X)\r\n",
    "\r\n",
    "  def predict_proba(self, X):\r\n",
    "    \"\"\"Return model probability predictions for dataset X\"\"\"\r\n",
    "    return self.model.predict_proba(X)\r\n",
    "\r\n",
    "  def show_summary(self):\r\n",
    "    self.model.show_summary()\r\n",
    "\r\n",
    "  def accuracy_score(self):\r\n",
    "    \"\"\"Returns average accuracy score from CV\"\"\"\r\n",
    "    return np.mean(self.accuracy_scores)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CV without early stopping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "softmaxCV = KfoldCV(Softmax())\r\n",
    "softmaxCV.fit(X, y, learn_rate=0.05, early_stop=False)\r\n",
    "print(\"Cross Validation accuracy: \", softmaxCV.accuracy_score())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross Validation accuracy:  0.9111111111111111\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CV with early stopping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "softmaxCV = KfoldCV(Softmax())\r\n",
    "softmaxCV.fit(X, y, learn_rate=0.05, early_stop=True, early_stop_thresh=300) # With early stop criteria\r\n",
    "print(\"Cross Validation accuracy: \", softmaxCV.accuracy_score())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross Validation accuracy:  0.9444444444444443\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "9da1c12f6d354dd36d308cb51e3aa17755429bd06bf828d149e4579c59bb3701"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}